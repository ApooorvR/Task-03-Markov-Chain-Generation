# Task 03 â€“ Text Generation using Markov Chains

## Objective
The objective of this task is to generate text using a probabilistic approach based on Markov Chains.

## Approach
A Markov Chain model was implemented to learn transition probabilities between words from a given text corpus. Based on the current word, the next word is predicted using learned probabilities, enabling text generation without deep learning models.

## Model Used
- Markov Chain (Statistical Language Model)

## Tools & Technologies
- Python
- Random and collections libraries

## Implementation Steps
1. Preprocessed the input text dataset
2. Built word-to-word transition probabilities
3. Implemented a Markov Chain generator
4. Generated text sequences starting from a given seed word

## Result
The model successfully generates text sequences that resemble the structure and style of the training data.

## Learning Outcome
- Understanding probabilistic text generation
- Difference between statistical and neural language models
- Fundamentals of language modeling without deep learning
